{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iNef0JpfkGS"
   },
   "source": [
    "# Lab 9.1: MultiQueryDemo\n",
    "\n",
    "## Tổng quan bài tập\n",
    "**Đề bài**: Ở bài Lab này, bạn sẽ được hướng dẫn các thao tác về việc Đọc dữ liệu Stream từ Kafka cũng như cách ghi dữ liệu ở cả dạng File Sink và Kafka Sink\n",
    "\n",
    "## Tài nguyên\n",
    "Do bài Lab này liên quan đến đến xử lý dữ liệu Stream với Kafka, vậy nên bạn sẽ cần cài đặt Kafka, bạn có thể tham khảo video sau về cách cài đặt:\n",
    "- [Cài đặt Kafka](https://funix.udemy.com/course/spark-streaming-using-python/learn/lecture/21955580#overview)\n",
    "\n",
    "Bạn sẽ cần tải các Kafka Script ở [link sau](https://drive.google.com/file/d/1RMiLzXVTiRlNsvfF3I63ylKVGjBXt7AK/view?usp=sharing) để có thể sử dụng Kafka.\n",
    "\n",
    "\n",
    "Bạn cũng sẽ cần tải các dữ liệu ở [link sau](https://drive.google.com/file/d/18RrmmF1h2-HS6wvteZ5lPcxs4iPCm5CB/view?usp=sharing) để có thể kiểm thử cho bài Lab. \n",
    "\n",
    "\n",
    "Ngoài ra, bạn có thể tham khảo các video sau trong trường hợp chưa hiểu cách làm bài Lab:\n",
    "- [Multi-query Streams Application](https://funix.udemy.com/course/spark-streaming-using-python/learn/lecture/21955620#overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkfVOOhpfkGV"
   },
   "source": [
    "Import các Package cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qE4QeXnyfkGW"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, IntegerType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JnFYd8AQfkGY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/09 20:50:15 WARN Utils: Your hostname, VNHCM1QANB017 resolves to a loopback address: 127.0.1.1; using 172.31.49.251 instead (on interface eth0)\n",
      "24/03/09 20:50:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/baodk/Programs/spark-3.5.1-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/baodk/.ivy2/cache\n",
      "The jars for the packages stored in: /home/baodk/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1607c9ff-7f31-484f-8453-4f2276642cf1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.13/3.5.1/spark-sql-kafka-0-10_2.13-3.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1!spark-sql-kafka-0-10_2.13.jar (4741ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.13/3.5.1/spark-token-provider-kafka-0-10_2.13-3.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1!spark-token-provider-kafka-0-10_2.13.jar (1596ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parallel-collections_2.13/1.0.4/scala-parallel-collections_2.13-1.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4!scala-parallel-collections_2.13.jar (4261ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (5388ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (1596ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (2543ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (10626ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (3333ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (4197ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (2702ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (8828ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (3249ms)\n",
      ":: resolution report :: resolve 79439ms :: artifacts dl 53071ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   12  |   12  |   0   ||   12  |   12  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1607c9ff-7f31-484f-8453-4f2276642cf1\n",
      "\tconfs: [default]\n",
      "\t12 artifacts copied, 0 already retrieved (57876kB/80ms)\n",
      "24/03/09 20:52:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "\t.builder \\\n",
    "\t.appName(\"lab9 Multi Query Demo\") \\\n",
    "\t.master(\"local[3]\") \\\n",
    "\t.config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\") \\\n",
    "\t.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-2moBfYfkGY"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "D-2moBfYfkGY",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///home/baodk/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar,file:///home/baodk/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar,file:///home/baodk/.ivy2/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar,file:///home/baodk/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar,file:///home/baodk/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar,file:///home/baodk/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar,file:///home/baodk/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar,file:///home/baodk/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar,file:///home/baodk/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar,file:///home/baodk/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar,file:///home/baodk/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar,file:///home/baodk/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar\n"
     ]
    }
   ],
   "source": [
    "print(spark._jsc.sc().getConf().get(\"spark.jars\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-2moBfYfkGY"
   },
   "source": [
    "Tạo schema cho dữ liệu đầu vào"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rZoMFgSxfkGZ"
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "\tStructField(\"InvoiceNumber\", StringType()),\n",
    "\tStructField(\"CreatedTime\", LongType()),\n",
    "\tStructField(\"StoreID\", StringType()),\n",
    "\tStructField(\"PosID\", StringType()),\n",
    "\tStructField(\"CashierID\", StringType()),\n",
    "\tStructField(\"CustomerType\", StringType()),\n",
    "\tStructField(\"CustomerCardNo\", StringType()),\n",
    "\tStructField(\"TotalAmount\", DoubleType()),\n",
    "\tStructField(\"NumberOfItems\", IntegerType()),\n",
    "\tStructField(\"PaymentMethod\", StringType()),\n",
    "\tStructField(\"CGST\", DoubleType()),\n",
    "\tStructField(\"SGST\", DoubleType()),\n",
    "\tStructField(\"CESS\", DoubleType()),\n",
    "\tStructField(\"DeliveryType\", StringType()),\n",
    "\tStructField(\"DeliveryAddress\", StructType([\n",
    "\t\tStructField(\"AddressLine\", StringType()),\n",
    "\t\tStructField(\"City\", StringType()),\n",
    "\t\tStructField(\"State\", StringType()),\n",
    "\t\tStructField(\"PinCode\", StringType()),\n",
    "\t\tStructField(\"ContactNumber\", StringType())\n",
    "\t])),\n",
    "\tStructField(\"InvoiceLineItems\", ArrayType(StructType([\n",
    "\t\tStructField(\"ItemCode\", StringType()),\n",
    "\t\tStructField(\"ItemDescription\", StringType()),\n",
    "\t\tStructField(\"ItemPrice\", DoubleType()),\n",
    "\t\tStructField(\"ItemQty\", IntegerType()),\n",
    "\t\tStructField(\"TotalValue\", DoubleType())\n",
    "\t]))),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvI8schCfkGa"
   },
   "source": [
    "Hãy hoàn thiện các phần `[...]` để hoàn thiện đoạn code và giải quyết bài toán theo yêu cầu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2kH64e2fkGb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/09 20:54:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/03/09 20:54:01 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/09 20:54:01 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "24/03/09 20:54:01 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Đọc dữ liệu từ Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "\t.format(\"kafka\") \\\n",
    "\t.option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "\t.option(\"subscribe\", \"invoices\") \\\n",
    "\t.option(\"startingOffsets\", \"earliest\") \\\n",
    "\t.load()\n",
    "\n",
    "# Chuyển dữ liệu từ dạng JSON về MapType()\n",
    "value_df = kafka_df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    "\n",
    "# Xử lý dữ liệu\n",
    "notification_df = value_df.select(\"value.InvoiceNumber\", \"value.CustomerCardNo\", \"value.TotalAmount\") \\\n",
    "\t.withColumn(\"EarnedLoyaltyPoints\", expr(\"TotalAmount * 0.2\"))\n",
    "\n",
    "# Ghi dữ liệu ở dạng Kafka Sink\n",
    "# Chuyển đổi Dataframe về dạng key - value\n",
    "kafka_target_df = notification_df.selectExpr(\"InvoiceNumber as key\", \"to_json(struct(*)) as value\")\n",
    "\n",
    "notification_writer_query = kafka_target_df \\\n",
    "\t.writeStream \\\n",
    "\t.queryName(\"Notification Writer\") \\\n",
    "\t.format(\"kafka\") \\\n",
    "\t.option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "\t.option(\"topic\", \"notifications\") \\\n",
    "\t.outputMode(\"append\") \\\n",
    "\t.option(\"checkpointLocation\", \"chk-point-dir/notify\") \\\n",
    "\t.start()\n",
    "\n",
    "# Trích xuất các dữ liệu\n",
    "explode_df = value_df.selectExpr(\"value.InvoiceNumber\", \"value.CreatedTime\", \"value.StoreID\",\n",
    "\t\t\t\t\t\t\t\t\t\"value.PosID\", \"value.CustomerType\", \"value.PaymentMethod\", \"value.DeliveryType\",\n",
    "\t\t\t\t\t\t\t\t\t\"value.DeliveryAddress.City\",\n",
    "\t\t\t\t\t\t\t\t\t\"value.DeliveryAddress.State\", \"value.DeliveryAddress.PinCode\",\n",
    "\t\t\t\t\t\t\t\t\t\"explode(value.InvoiceLineItems) as LineItem\")\n",
    "\n",
    "flattened_df = explode_df \\\n",
    "\t.withColumn(\"ItemCode\", expr(\"LineItem.ItemCode\")) \\\n",
    "\t.withColumn(\"ItemDescription\", expr(\"LineItem.ItemDescription\")) \\\n",
    "\t.withColumn(\"ItemPrice\", expr(\"LineItem.ItemPrice\")) \\\n",
    "\t.withColumn(\"ItemQty\", expr(\"LineItem.ItemQty\")) \\\n",
    "\t.withColumn(\"TotalValue\", expr(\"LineItem.TotalValue\")) \\\n",
    "\t.drop(\"LineItem\")\n",
    "\n",
    "\n",
    "# Ghi dữ liệu dưới dạng File Sink\n",
    "invoice_writer_query = flattened_df.writeStream \\\n",
    "\t.format(\"json\") \\\n",
    "\t.queryName(\"Flattened Invoice Writer\") \\\n",
    "\t.outputMode(\"append\") \\\n",
    "\t.option(\"path\", \"output\") \\\n",
    "\t.option(\"checkpointLocation\", \"chk-point-dir/flatten\") \\\n",
    "\t.start()\n",
    "\n",
    "print(\"Waiting for Queries\")\n",
    "spark.streams.awaitAnyTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 9.1: MultiQueryDemo.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "67e442376543da31be6bdf2ca3edab58a905f904bfaaefbdb9d9ed4ba9c6bc77"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
