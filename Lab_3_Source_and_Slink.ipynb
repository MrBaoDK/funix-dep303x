{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaL70vgSqsqe"
      },
      "source": [
        "# Lab 3: Source and Slink\n",
        "\n",
        "## Tổng quan bài tập\n",
        "**Đề bài**: Dựa vào tập dữ liệu, hãy đọc dữ liệu từ Dataset và tạo Schema hợp lý và phân vùng lại dữ liệu. Hãy hoàn thiện các phần `[...]` để hoàn thiện đoạn code và giải quyết bài toán trên.\n",
        "\n",
        "## Tài nguyên tham khảo\n",
        "\n",
        "Bạn có thể tải tập Dataset tại [link sau](https://drive.google.com/file/d/1X-6xQafkMwDU39xBfEdItDwqg8N_6WV9/view?usp=sharing). Sau đó đưa lên Google Drive và kết nối với Colab là có thể sử dụng được. Tập dữ liệu là file .csv sẽ có cấu trúc như sau:\n",
        "```\n",
        "root\n",
        " |-- FL_DATE: date (nullable = true)\n",
        " |-- OP_CARRIER: string (nullable = true)\n",
        " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
        " |-- ORIGIN: string (nullable = true)\n",
        " |-- ORIGIN_CITY_NAME: string (nullable = true)\n",
        " |-- DEST: string (nullable = true)\n",
        " |-- DEST_CITY_NAME: string (nullable = true)\n",
        " |-- CRS_DEP_TIME: integer (nullable = true)\n",
        " |-- DEP_TIME: integer (nullable = true)\n",
        " |-- WHEELS_ON: integer (nullable = true)\n",
        " |-- TAXI_IN: integer (nullable = true)\n",
        " |-- CRS_ARR_TIME: integer (nullable = true)\n",
        " |-- ARR_TIME: integer (nullable = true)\n",
        " |-- CANCELLED: integer (nullable = true)\n",
        " |-- DISTANCE: integer (nullable = true)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPRu5D4IfO3I"
      },
      "source": [
        "# Cài đặt Spark trên Google Colab\n",
        "\n",
        "Để có thể sử dụng Spark trên môi trường Google Colab thì bạn sẽ cần cài đặt một số thành phần sau:\n",
        "- Java 8\n",
        "- Spark Binary\n",
        "- findspark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHdKMp8zeMel",
        "outputId": "79f815dd-2290-428b-bf5c-07197d47cfb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [48.6 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,046 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,305 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,326 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,599 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,256 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,158 kB]\n",
            "Fetched 8,989 kB in 3s (3,093 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "40 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fv63ddnSOG-"
      },
      "source": [
        "Sau đó, bạn sẽ cần khai báo cho hệ thống các đường dẫn cho các thành phần vừa cài."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8gN9Zhx8vPb"
      },
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM4wpiJMTCib"
      },
      "source": [
        "# Kết nối với Google Drive\n",
        "\n",
        "Để lấy dữ liệu từ các Dataset, bạn sẽ phải lưu file dữ liệu lên Google Drive. Sau đó kết nối Colab đến Google Drive của bạn và lấy được các file dữ liệu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoYnrNV7UChX",
        "outputId": "12dca1e1-370b-41b2-b892-83f21c1e8bd3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0yzaQQNDKoV"
      },
      "source": [
        "# Source and Slink\n",
        "\n",
        "Bạn sẽ cần khởi tạo 1 SparkSesson để có thể bắt đầu Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjqT5XXj80P8"
      },
      "source": [
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, DateType, StringType, IntegerType\n",
        "from pyspark.sql.functions import spark_partition_id\n",
        "\n",
        "conf = SparkConf() \\\n",
        "    .setMaster('local') \\\n",
        "    .setAppName('lab3')\n",
        "\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "DATASET_PATH = '/content/gdrive/My Drive/DEP303/flight-time.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q12Bu68-wkqq"
      },
      "source": [
        "Đọc dữ liệu bằng cách sử dụng StructType Schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyrh2wUTwgXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d8315a-7998-4aaf-c685-791f8b1ae681"
      },
      "source": [
        "flightSchemaStruct = StructType([\n",
        "  StructField('FL_DATE', DateType()),\n",
        "  StructField('OP_CARRIER', StringType()),\n",
        "  StructField('OP_CARRIER_FL_NUM', IntegerType()),\n",
        "  StructField('ORIGIN', StringType()),\n",
        "  StructField('ORIGIN_CITY_NAME', StringType()),\n",
        "  StructField('DEST', StringType()),\n",
        "  StructField('DEST_CITY_NAME', StringType()),\n",
        "  StructField('CRS_DEP_TIME', IntegerType()),\n",
        "  StructField('DEP_TIME', IntegerType()),\n",
        "  StructField('WHEELS_ON', IntegerType()),\n",
        "  StructField('TAXI_IN', IntegerType()),\n",
        "  StructField('CRS_ARR_TIME', IntegerType()),\n",
        "  StructField('ARR_TIME', IntegerType()),\n",
        "  StructField('CANCELLED', IntegerType()),\n",
        "  StructField('DISTANCE', IntegerType())\n",
        "]);\n",
        "\n",
        "print('Schema by StructType')\n",
        "flightTimeCsvDF = spark.read \\\n",
        "    .format(\"CSV\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(flightSchemaStruct) \\\n",
        "    .option(\"mode\", \"FAILFAST\") \\\n",
        "    .option(\"dateFormat\", \"M/d/yyyy\") \\\n",
        "    .load(DATASET_PATH);\n",
        "\n",
        "flightTimeCsvDF.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema by StructType\n",
            "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
            "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "|2000-01-01|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
            "|2000-01-01|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
            "|2000-01-01|        DL|             1857|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1415|    1414|     1642|      9|        1721|    1651|        0|     946|\n",
            "|2000-01-01|        DL|             1997|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1715|    1720|     1955|     10|        2013|    2005|        0|     946|\n",
            "|2000-01-01|        DL|             2065|   BOS|      Boston, MA| ATL|   Atlanta, GA|        2015|    2010|     2230|     10|        2300|    2240|        0|     946|\n",
            "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiSmnmH8wwc2"
      },
      "source": [
        "Đọc dữ liệu bằng cách sử dụng String Schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAvU7CF7sVXg",
        "outputId": "6589c305-c54e-48f9-8fa4-d37ba67ebb6f"
      },
      "source": [
        "flightSchemaDDL = \"\"\"\n",
        "  FL_DATE DATE,\n",
        "  OP_CARRIER STRING,\n",
        "  OP_CARRIER_FL_NUM INT,\n",
        "  ORIGIN STRING,\n",
        "  ORIGIN_CITY_NAME STRING,\n",
        "  DEST STRING,\n",
        "  DEST_CITY_NAME STRING,\n",
        "  CRS_DEPT_TIME INT,\n",
        "  DEP_TIME INT,\n",
        "  WHEELS_ON INT,\n",
        "  TAXI_IN INT,\n",
        "  CRS_ARR_TIME INT,\n",
        "  ARR_TIME INT,\n",
        "  CANCELLED INT,\n",
        "  DISTANCE INT\n",
        "\"\"\"\n",
        "\n",
        "print('Schema by String')\n",
        "flightTimeCsvDF = spark.read \\\n",
        "    .format(\"CSV\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(flightSchemaDDL) \\\n",
        "    .option(\"mode\", \"FAILFAST\") \\\n",
        "    .option(\"dateFormat\", \"M/d/yyyy\") \\\n",
        "    .load(DATASET_PATH)\n",
        "\n",
        "\n",
        "flightTimeCsvDF.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema by String\n",
            "+----------+----------+-----------------+------+----------------+----+--------------+-------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEPT_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
            "+----------+----------+-----------------+------+----------------+----+--------------+-------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "|2000-01-01|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|         1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
            "|2000-01-01|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|         1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
            "|2000-01-01|        DL|             1857|   BOS|      Boston, MA| ATL|   Atlanta, GA|         1415|    1414|     1642|      9|        1721|    1651|        0|     946|\n",
            "|2000-01-01|        DL|             1997|   BOS|      Boston, MA| ATL|   Atlanta, GA|         1715|    1720|     1955|     10|        2013|    2005|        0|     946|\n",
            "|2000-01-01|        DL|             2065|   BOS|      Boston, MA| ATL|   Atlanta, GA|         2015|    2010|     2230|     10|        2300|    2240|        0|     946|\n",
            "+----------+----------+-----------------+------+----------------+----+--------------+-------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTkFiQ8a0104"
      },
      "source": [
        "Tiếp theo đó bạn cần phải phân vùng lại cho dữ liệu và lưu dữ liệu đó ra file. Đầu tiên hãy phân vùng dữ liệu thành **5** vùng."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dccanhlM0O4S",
        "outputId": "97d47a40-fc36-479a-eb11-769ccde95298"
      },
      "source": [
        "flightTimeCsvDF.groupBy(spark_partition_id()).count().show()\n",
        "print(\"Num Partitions before: \" + str(flightTimeCsvDF.rdd.getNumPartitions()))\n",
        "\n",
        "\n",
        "partitionedDF = flightTimeCsvDF.repartition(5)\n",
        "print(\"Num Partitions after: \" + str(partitionedDF.rdd.getNumPartitions()))\n",
        "partitionedDF.groupBy(spark_partition_id()).count().show()\n",
        "\n",
        "partitionedDF.write \\\n",
        "    .format(\"json\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"path\", \"./dataSink/avro/\") \\\n",
        "    .save()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+\n",
            "|SPARK_PARTITION_ID()| count|\n",
            "+--------------------+------+\n",
            "|                   0|470477|\n",
            "+--------------------+------+\n",
            "\n",
            "Num Partitions before: 1\n",
            "Num Partitions after: 5\n",
            "+--------------------+-----+\n",
            "|SPARK_PARTITION_ID()|count|\n",
            "+--------------------+-----+\n",
            "|                   0|94096|\n",
            "|                   1|94095|\n",
            "|                   2|94095|\n",
            "|                   3|94095|\n",
            "|                   4|94096|\n",
            "+--------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_ewPuoD11iL"
      },
      "source": [
        "Ngoài ra, bạn có có thể phân vùng lại theo các trường. Hãy hoàn thiện đoạn code để phân vùng theo hai trường là `\"OP_CARRIER\"` và `\"ORIGIN\"`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGTHBSRnyVLq"
      },
      "source": [
        "flightTimeCsvDF.write \\\n",
        "    .format(\"json\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"path\", \"dataSink/json/\") \\\n",
        "    .partitionBy([\"OP_CARRIER\", \"ORIGIN\"]) \\\n",
        "    .option(\"maxRecordsPerFile\", 10000) \\\n",
        "    .save()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}